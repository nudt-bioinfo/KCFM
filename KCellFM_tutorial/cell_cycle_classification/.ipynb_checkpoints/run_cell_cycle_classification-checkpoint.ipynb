{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bf323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the root directory of the project\n",
    "project_root = Path(\"/home/lxz/scmamba/KCellFM_tutorial/cell_cycle_classification\").parent.parent\n",
    "# project_root = Path(__file__).parent.parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7de180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxz/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.model import MambaModel\n",
    "from models.gene_tokenizer import GeneVocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import anndata\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5638e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed random seeds ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ef9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "epochs = 50  # 50 epochs\n",
    "batch_size = 64\n",
    "embsize = 512\n",
    "nhead = 8\n",
    "d_hid = 512\n",
    "nlayers = 6\n",
    "dropout = 0.1\n",
    "lr = 1e-5\n",
    "pad_token = \"<pad>\"\n",
    "max_seq_len = 8192\n",
    "input_emb_style = \"continuous\"\n",
    "cell_emb_style = \"cls\"\n",
    "mask_value = -1\n",
    "pad_value = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7356918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary list\n",
    "vocab = GeneVocab.from_file(\"/home/lxz/scmamba/vocab.json\")\n",
    "ntokens = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1a4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Mapping\n",
    "label_to_id = {\n",
    "    'single H1-Fucci cell sorted from G1 phase of the cell cycle only': 0,\n",
    "    'single H1-Fucci cell sorted from S phase of the cell cycle only': 1,\n",
    "    'single H1-Fucci cell sorted from G2/M phase of the cell cycle only': 2\n",
    "}\n",
    "class_num = len(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932e948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellCycleDataset(Dataset):\n",
    "    def __init__(self, expr_matrix, labels):\n",
    "        self.expr_matrix = expr_matrix\n",
    "        self.labels = labels\n",
    "        self.cell_ids = list(labels.index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cell_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cell_id = self.cell_ids[idx]\n",
    "        # Obtain gene expression data\n",
    "        expr_values = self.expr_matrix.loc[cell_id].values  # Ensure continuous memory\n",
    "        gene_names = self.expr_matrix.columns\n",
    "        \n",
    "        # Only retain genes with non-zero expression values\n",
    "        non_zero_indices = np.where(expr_values != 0)[0]\n",
    "        expr_values = expr_values[non_zero_indices]\n",
    "        gene_names = gene_names[non_zero_indices]\n",
    "\n",
    "        # Map gene names to IDs and filter expression values simultaneously\n",
    "        filtered_gene_ids = []\n",
    "        filtered_expr_values = []\n",
    "        for gene, value in zip(gene_names, expr_values):\n",
    "            if gene in vocab:  # Only keep genes in the vocabulary list\n",
    "                filtered_gene_ids.append(vocab[gene])\n",
    "                filtered_expr_values.append(value)\n",
    "        gene_ids = filtered_gene_ids\n",
    "        expr_values = filtered_expr_values\n",
    "        \n",
    "        # Add CLS token\n",
    "        if len(gene_ids) > max_seq_len:\n",
    "            idx = np.random.choice(len(gene_ids), max_seq_len, replace=False)\n",
    "            gene_ids = [gene_ids[i] for i in idx]\n",
    "            expr_values = [expr_values[i] for i in idx]\n",
    "             \n",
    "        # Fill sequence\n",
    "        padding_length = max_seq_len - len(gene_ids)\n",
    "        if padding_length > 0:\n",
    "            gene_ids = gene_ids + [vocab[\"<pad>\"]] * padding_length\n",
    "            expr_values = expr_values + [pad_value] * padding_length\n",
    "        gene_ids = [vocab[\"<cls>\"]] + gene_ids\n",
    "        expr_values = np.concatenate([[0.0], expr_values]) \n",
    "        \n",
    "        # Create padding mask\n",
    "        padding_mask = [False] * (max_seq_len+1)\n",
    "        for i in range(len(padding_mask)):\n",
    "            if gene_ids[i] == vocab[\"<pad>\"]:\n",
    "                padding_mask[i] = True\n",
    "        \n",
    "        # Get tags\n",
    "        label = label_to_id[self.labels[cell_id]]\n",
    "        return {\n",
    "            'gene_ids': torch.LongTensor(gene_ids),\n",
    "            'expr_values': torch.FloatTensor(expr_values),\n",
    "            'padding_mask': torch.BoolTensor(padding_mask),\n",
    "            'label': torch.as_tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b8b62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # load dataset\n",
    "    expr_matrix = pd.read_csv(\"/home/lxz/scmamba/细胞周期/h-ESC/GSE64016_logp_10k.csv\", index_col=0)\n",
    "    adata = anndata.read_h5ad(\"/home/lxz/scmamba/细胞周期/GSE64016_adata.h5ad\")\n",
    "    expr_matrix.index = adata.obs.index\n",
    "    labels = pd.Series(adata.obs['source_name_ch1'], index=adata.obs.index)\n",
    "    mask = labels != 'single H1 hESC'\n",
    "    filtered_expr_matrix = expr_matrix[mask]\n",
    "    filtered_labels = labels[mask]\n",
    "\n",
    "    # Divide the training and test set\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(filtered_labels)),\n",
    "        test_size=0.3,\n",
    "        stratify=filtered_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = CellCycleDataset(filtered_expr_matrix, filtered_labels)\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(full_dataset, test_idx)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MambaModel(\n",
    "        ntokens, embsize, nhead, d_hid, nlayers,\n",
    "        vocab=vocab, dropout=dropout, pad_token=pad_token,\n",
    "        pad_value=pad_value, input_emb_style=input_emb_style,\n",
    "        cell_emb_style=cell_emb_style, class_num=class_num\n",
    "    ).to(device)\n",
    "\n",
    "    # Load pre-trained weights (skip classification header)\n",
    "    try:\n",
    "        pretrained_dict = torch.load(\"/home/lxz/scmamba/model_state/cell_cls_3loss_6layer_final.pth\",\n",
    "                                    map_location=device)\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                        if k in model_dict and v.shape == model_dict[k].shape\n",
    "                        and not k.startswith('cls_decoder')}  # Exclude all category header parameters\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"Successfully loaded pre training weights (excluding classification head weights)\")\n",
    "\n",
    "        # Reinitialize the classification header\n",
    "        print(\"Initialize the classification header weights...\")\n",
    "        nn.init.kaiming_normal_(model.cls_decoder.out_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if model.cls_decoder.out_layer.bias is not None:\n",
    "            nn.init.zeros_(model.cls_decoder.out_layer.bias)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load pre-training weights: {str(e)}\")\n",
    "\n",
    "    # Freeze and unfreeze logic\n",
    "    print(f\"Freeze the first {nlayers - 3} layer, unfreeze the last 3 layers and classifier\")\n",
    "\n",
    "    # First, freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last two layers of Mamba layer\n",
    "    for i in range(nlayers - 3, nlayers):\n",
    "        layer = model.mamba_encoder[i]\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(f\"Unfreeze Mamba layers {i}: Parameters trainable\")\n",
    "\n",
    "    # Unfreeze classification header\n",
    "    for param in model.cls_decoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"The classifier parameters have been set to trainable\")\n",
    "\n",
    "    # Print trainable parameters\n",
    "    print(\"\\nTrainable parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "\n",
    "    # Only optimize the parameters that need to be trained\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"\\nNumber of trainable parameters: {sum(p.numel() for p in trainable_params)}\")\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Record the training process\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            gene_ids = batch['gene_ids'].to(device)\n",
    "            expr_values = batch['expr_values'].to(device)\n",
    "            padding_mask = batch['padding_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    src=gene_ids,\n",
    "                    values=expr_values,\n",
    "                    src_key_padding_mask=padding_mask\n",
    "                )\n",
    "            loss = criterion(outputs[\"cls_output\"], labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Record training loss\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Test set evaluation\n",
    "        model.eval()\n",
    "        test_preds, test_labels = [], []\n",
    "        all_cell_embs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                gene_ids = batch['gene_ids'].to(device)\n",
    "                expr_values = batch['expr_values'].to(device)\n",
    "                padding_mask = batch['padding_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(src=gene_ids, values=expr_values, src_key_padding_mask=padding_mask)\n",
    "                _, predicted = torch.max(outputs[\"cls_output\"], 1)\n",
    "                cell_embs = outputs[\"cell_emb\"].cpu().numpy()\n",
    "                test_preds.extend(predicted.cpu().numpy())\n",
    "                test_labels.extend(labels.cpu().numpy())\n",
    "                all_cell_embs.append(cell_embs)\n",
    "        \n",
    "        test_accuracy = 100 * np.sum(np.array(test_preds) == np.array(test_labels)) / len(test_labels)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        all_cell_embs = np.concatenate(all_cell_embs, axis=0)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save model and results\n",
    "    torch.save(model.state_dict(), '/home/lxz/scmamba/model_state/mamba_cell_cycle_final_ipynb.pth')\n",
    "    np.save(\"/home/lxz/scmamba/细胞周期/cell_embeddings_test.npy\", all_cell_embs)\n",
    "    \n",
    "    predictions = {\n",
    "        'preds': test_preds,\n",
    "        'labels': test_labels,\n",
    "    }\n",
    "    \n",
    "    with open('/home/lxz/scmamba/细胞周期/labels_preds_test.pkl', 'wb') as f:\n",
    "        pickle.dump(predictions, f)\n",
    "    \n",
    "    # Draw training curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), test_accuracies, 'b-o', label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Test Accuracy vs. Epoch')\n",
    "    plt.xticks(np.arange(1, epochs+1, step=max(1, epochs//10)))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # loss curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_losses, 'r-o', label='Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train Loss vs. Epoch')\n",
    "    plt.xticks(np.arange(1, epochs+1, step=max(1, epochs//10)))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/home/lxz/scmamba/细胞周期/training_curve_ipynb.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Output the final result\n",
    "    print(\"\\n===== Final Test Results =====\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "    print(classification_report(\n",
    "        test_labels,\n",
    "        test_preds,\n",
    "        target_names=list(label_to_id.keys()),\n",
    "        digits=4\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5c98c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded pre training weights (excluding classification head weights)\n",
      "Initialize the classification header weights...\n",
      "Freeze the first 3 layer, unfreeze the last 3 layers and classifier\n",
      "Unfreeze Mamba layers 3: Parameters trainable\n",
      "Unfreeze Mamba layers 4: Parameters trainable\n",
      "Unfreeze Mamba layers 5: Parameters trainable\n",
      "The classifier parameters have been set to trainable\n",
      "\n",
      "Trainable parameters:\n",
      "encoder.embedding.weight\n",
      "encoder.enc_norm.weight\n",
      "encoder.enc_norm.bias\n",
      "value_encoder.linear1.weight\n",
      "value_encoder.linear1.bias\n",
      "value_encoder.linear2.weight\n",
      "value_encoder.linear2.bias\n",
      "value_encoder.norm.weight\n",
      "value_encoder.norm.bias\n",
      "cls_decoder._decoder.0.weight\n",
      "cls_decoder._decoder.0.bias\n",
      "cls_decoder._decoder.2.weight\n",
      "cls_decoder._decoder.2.bias\n",
      "cls_decoder._decoder.3.weight\n",
      "cls_decoder._decoder.3.bias\n",
      "cls_decoder._decoder.5.weight\n",
      "cls_decoder._decoder.5.bias\n",
      "cls_decoder.out_layer.weight\n",
      "cls_decoder.out_layer.bias\n",
      "decoder.fc.0.weight\n",
      "decoder.fc.0.bias\n",
      "decoder.fc.2.weight\n",
      "decoder.fc.2.bias\n",
      "decoder.fc.4.weight\n",
      "decoder.fc.4.bias\n",
      "mamba_encoder.0.mamba_fwd.dt_bias\n",
      "mamba_encoder.0.mamba_fwd.A_log\n",
      "mamba_encoder.0.mamba_fwd.D\n",
      "mamba_encoder.0.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.0.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.0.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.0.mamba_fwd.norm.weight\n",
      "mamba_encoder.0.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.0.mamba_rev.dt_bias\n",
      "mamba_encoder.0.mamba_rev.A_log\n",
      "mamba_encoder.0.mamba_rev.D\n",
      "mamba_encoder.0.mamba_rev.conv1d.weight\n",
      "mamba_encoder.0.mamba_rev.conv1d.bias\n",
      "mamba_encoder.0.mamba_rev.norm.weight\n",
      "mamba_encoder.1.mamba_fwd.dt_bias\n",
      "mamba_encoder.1.mamba_fwd.A_log\n",
      "mamba_encoder.1.mamba_fwd.D\n",
      "mamba_encoder.1.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.1.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.1.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.1.mamba_fwd.norm.weight\n",
      "mamba_encoder.1.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.1.mamba_rev.dt_bias\n",
      "mamba_encoder.1.mamba_rev.A_log\n",
      "mamba_encoder.1.mamba_rev.D\n",
      "mamba_encoder.1.mamba_rev.conv1d.weight\n",
      "mamba_encoder.1.mamba_rev.conv1d.bias\n",
      "mamba_encoder.1.mamba_rev.norm.weight\n",
      "mamba_encoder.2.mamba_fwd.dt_bias\n",
      "mamba_encoder.2.mamba_fwd.A_log\n",
      "mamba_encoder.2.mamba_fwd.D\n",
      "mamba_encoder.2.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.2.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.2.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.2.mamba_fwd.norm.weight\n",
      "mamba_encoder.2.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.2.mamba_rev.dt_bias\n",
      "mamba_encoder.2.mamba_rev.A_log\n",
      "mamba_encoder.2.mamba_rev.D\n",
      "mamba_encoder.2.mamba_rev.conv1d.weight\n",
      "mamba_encoder.2.mamba_rev.conv1d.bias\n",
      "mamba_encoder.2.mamba_rev.norm.weight\n",
      "mamba_encoder.3.mamba_fwd.dt_bias\n",
      "mamba_encoder.3.mamba_fwd.A_log\n",
      "mamba_encoder.3.mamba_fwd.D\n",
      "mamba_encoder.3.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.3.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.3.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.3.mamba_fwd.norm.weight\n",
      "mamba_encoder.3.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.3.mamba_rev.dt_bias\n",
      "mamba_encoder.3.mamba_rev.A_log\n",
      "mamba_encoder.3.mamba_rev.D\n",
      "mamba_encoder.3.mamba_rev.conv1d.weight\n",
      "mamba_encoder.3.mamba_rev.conv1d.bias\n",
      "mamba_encoder.3.mamba_rev.norm.weight\n",
      "mamba_encoder.4.mamba_fwd.dt_bias\n",
      "mamba_encoder.4.mamba_fwd.A_log\n",
      "mamba_encoder.4.mamba_fwd.D\n",
      "mamba_encoder.4.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.4.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.4.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.4.mamba_fwd.norm.weight\n",
      "mamba_encoder.4.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.4.mamba_rev.dt_bias\n",
      "mamba_encoder.4.mamba_rev.A_log\n",
      "mamba_encoder.4.mamba_rev.D\n",
      "mamba_encoder.4.mamba_rev.conv1d.weight\n",
      "mamba_encoder.4.mamba_rev.conv1d.bias\n",
      "mamba_encoder.4.mamba_rev.norm.weight\n",
      "mamba_encoder.5.mamba_fwd.dt_bias\n",
      "mamba_encoder.5.mamba_fwd.A_log\n",
      "mamba_encoder.5.mamba_fwd.D\n",
      "mamba_encoder.5.mamba_fwd.in_proj.weight\n",
      "mamba_encoder.5.mamba_fwd.conv1d.weight\n",
      "mamba_encoder.5.mamba_fwd.conv1d.bias\n",
      "mamba_encoder.5.mamba_fwd.norm.weight\n",
      "mamba_encoder.5.mamba_fwd.out_proj.weight\n",
      "mamba_encoder.5.mamba_rev.dt_bias\n",
      "mamba_encoder.5.mamba_rev.A_log\n",
      "mamba_encoder.5.mamba_rev.D\n",
      "mamba_encoder.5.mamba_rev.conv1d.weight\n",
      "mamba_encoder.5.mamba_rev.conv1d.bias\n",
      "mamba_encoder.5.mamba_rev.norm.weight\n",
      "\n",
      "Number of trainable parameters: 42759748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|                                                                                                                                                      | 0/3 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacity of 47.54 GiB of which 221.56 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 45.41 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpr_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_output\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels)\n\u001b[1;32m    119\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scmamba/models/model.py:256\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[0;34m(self, src, values, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, values: Tensor, src_key_padding_mask: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m--> 256\u001b[0m     transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     output \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    258\u001b[0m     mlm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(transformer_output)\n",
      "File \u001b[0;32m~/scmamba/models/model.py:237\u001b[0m, in \u001b[0;36mMambaModel._encode\u001b[0;34m(self, src, values, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# 手动调用每个 BiMambaWrapper 的 forward 方法\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmamba_encoder:\n\u001b[0;32m--> 237\u001b[0m     total_embs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_embs\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scmamba/models/model.py:103\u001b[0m, in \u001b[0;36mBiMambaWrapper.forward\u001b[0;34m(self, hidden_states, src_key_padding_mask, inference_params)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states, hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m--> 103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Bidirectional processing\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Create default mask if not provided (all False, meaning no padding)\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/mamba_ssm/modules/mamba2.py:185\u001b[0m, in \u001b[0;36mMamba2.forward\u001b[0;34m(self, u, seqlen, seq_idx, cu_seqlens, inference_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m dt_limit_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(dt_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_limit)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mem_eff_path \u001b[38;5;129;01mand\u001b[39;00m inference_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmamba_split_conv1d_scan_combined\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md 1 w -> d w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(h p) -> h p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaddim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_has_hdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmsnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmsnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_has_hdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_before_gate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdt_limit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seqlen_og \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l d -> (b l) d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:930\u001b[0m, in \u001b[0;36mmamba_split_conv1d_scan_combined\u001b[0;34m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmamba_split_conv1d_scan_combined\u001b[39m(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seq_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dt_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)), return_final_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m, rmsnorm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rmsnorm_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, outproj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, outproj_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ngroups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, norm_before_gate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    Argument:\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        zxbcdt: (batch, seqlen, 2 * dim + 2 * ngroups * dstate + nheads) where dim == nheads * headdim\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m        out: (batch, seqlen, dim)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMambaSplitConv1dScanCombinedFn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzxbcdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1d_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_final_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrmsnorm_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutproj_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_before_gate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:115\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:795\u001b[0m, in \u001b[0;36mMambaSplitConv1dScanCombinedFn.forward\u001b[0;34m(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)\u001b[0m\n\u001b[1;32m    793\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([_swiglu_fwd(zx0), out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 795\u001b[0m     out_x, _, dt_out, dA_cumsum, states, final_states \u001b[38;5;241m=\u001b[39m \u001b[43m_mamba_chunk_scan_combined_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_softplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# reshape input data into 2D tensor\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     x_rms \u001b[38;5;241m=\u001b[39m rearrange(out_x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s h p -> (b s) (h p)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:324\u001b[0m, in \u001b[0;36m_mamba_chunk_scan_combined_fwd\u001b[0;34m(x, dt, A, B, C, chunk_size, D, z, dt_bias, initial_states, seq_idx, cu_seqlens, dt_softplus, dt_limit)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# states_tmp0 = rearrange(_state_passing_fwd(rearrange(states_tmp0, \"... p n -> ... (p n)\"), dA_cumsum_tmp0[:, :, :, -1], chunk_size=chunk_size), \"... (p n) -> ... p n\", n=dstate)\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# states_tmp1 = rearrange(_state_passing_fwd(rearrange(states_tmp1, \"... p n -> ... (p n)\"), dA_cumsum_tmp1[:, :, :, -1], chunk_size=chunk_size), \"... (p n) -> ... p n\", n=dstate)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m CB \u001b[38;5;241m=\u001b[39m _bmm_chunk_fwd(C, B, chunk_size, seq_idx\u001b[38;5;241m=\u001b[39mseq_idx, output_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 324\u001b[0m out, out_x \u001b[38;5;241m=\u001b[39m \u001b[43m_chunk_scan_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdA_cumsum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cu_seqlens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, out_x, dt, dA_cumsum, states, final_states\n",
      "File \u001b[0;32m~/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_chunk_scan.py:1247\u001b[0m, in \u001b[0;36m_chunk_scan_fwd\u001b[0;34m(cb, x, dt, dA_cumsum, C, states, D, z, seq_idx)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m seq_idx\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch, seqlen)\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# Allocates output.\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m z \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m     out_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(batch, seqlen, nheads, headdim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacity of 47.54 GiB of which 221.56 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 45.41 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c76041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a08f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
