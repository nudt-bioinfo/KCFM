# import torch
# from transformers import AutoTokenizer, AutoModel

# # åŠ è½½é…ç½®
# checkpoint = torch.load(
#     "/home/lxz/PubMedbert_finetuned/full_model.pth", 
#     weights_only=True  # å¯ç”¨å®‰å…¨æ¨¡å¼
# )
# model_path = checkpoint['config']['model_path']

# # åˆå§‹åŒ–åŸå§‹æ¨¡å‹å’Œè‡ªå®šä¹‰æ¨¡å‹
# tokenizer = AutoTokenizer.from_pretrained("/home/lxz/PubMedbert_finetuned/")  # åŠ è½½ä¿å­˜çš„tokenizer
# bert_model = AutoModel.from_pretrained(model_path)

# # é‡å»ºå®Œæ•´æ¨¡å‹
# class TripletPubMedBERT(torch.nn.Module):
#     def __init__(self, bert_model):
#         super().__init__()
#         self.bert = bert_model
#         self.rel_emb = torch.nn.Embedding(8, 768)
        
# model = TripletPubMedBERT(bert_model).to("cuda")
# model.load_state_dict(checkpoint['bert_state'], strict=False)  # åŠ è½½BERTéƒ¨åˆ†
# model.rel_emb.load_state_dict(checkpoint['rel_emb_state'])  # åŠ è½½å…³ç³»åµŒå…¥å±‚

# def get_word_embedding(word):
#     inputs = tokenizer(word, return_tensors="pt").to("cuda")
#     with torch.no_grad():
#         outputs = model.bert(**inputs)
#         # ä½¿ç”¨å‡å€¼æ± åŒ–
#         mask = inputs['attention_mask'].unsqueeze(-1)
#         embeddings = outputs.last_hidden_state * mask
#         pooled = embeddings.sum(dim=1) / mask.sum(dim=1)
#     return pooled.cpu().numpy()

# # ç¤ºä¾‹ï¼šè·å–"neuron"çš„embedding
# embedding = get_word_embedding("neuron")
# print(embedding)
# print(embedding.shape)  # åº”è¾“å‡º (1, 768)




from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# é…ç½®è®¾ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"
base_model_path = "/home/lxz/PubMedbert"
finetuned_path = "/home/lxz/PubMedbert_finetuned/full_model.pth"

# åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModel.from_pretrained(base_model_path).to(device)

# å®šä¹‰å¾®è°ƒæ¨¡å‹ç»“æ„
class FineTunedModel(torch.nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.bert = base_model  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹å®ä¾‹
        self.rel_emb = torch.nn.Embedding(8, 768)
        
    def forward(self, **inputs):
        return self.bert(**inputs)

# åŠ è½½å¾®è°ƒæ¨¡å‹ï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼‰
finetuned_model = FineTunedModel(base_model).to(device)  # å…±äº«åŸºç¡€æ¨¡å‹ç»“æ„

# è°ƒè¯•ï¼šæ£€æŸ¥checkpointå†…å®¹
checkpoint = torch.load(finetuned_path, map_location=device)
print("Checkpoint keys:", checkpoint.keys())  # ç¡®è®¤åŒ…å«'bert_state'

# ä¿®æ­£é”®åæ˜ å°„ï¼ˆæ ¹æ®å®é™…checkpointè°ƒæ•´ï¼‰
adjusted_state_dict = {
    k.replace('module.bert.', 'bert.').replace('bert.', ''): v 
    for k, v in checkpoint['bert_state'].items()
}

# ä¸¥æ ¼åŠ è½½å‚æ•°å¹¶æ£€æŸ¥ç¼ºå¤±é”®
load_result = finetuned_model.load_state_dict(adjusted_state_dict, strict=False)
print("\nå‚æ•°åŠ è½½æƒ…å†µ:")
print("Missing keys:", load_result.missing_keys)
print("Unexpected keys:", load_result.unexpected_keys)

# éªŒè¯å‚æ•°å·®å¼‚ï¼ˆå…³é”®æ­¥éª¤ï¼‰
print("\nå‚æ•°å·®å¼‚éªŒè¯:")
with torch.no_grad():
    # éšæœºé€‰å–10ä¸ªå‚æ•°æ¯”è¾ƒ
    for name, param in base_model.named_parameters():
        if name in adjusted_state_dict:
            diff = (param - adjusted_state_dict[name]).abs().max().item()
            if diff > 1e-6:
                print(f"ğŸ”´ {name}: å­˜åœ¨å·®å¼‚ (max_diff={diff:.6f})")
            else:
                print(f"ğŸŸ¢ {name}: æ— å·®å¼‚")
        else:
            print(f"âš ï¸ {name} ä¸åœ¨å¾®è°ƒæ¨¡å‹ä¸­")

# å‡å€¼æ± åŒ–å‡½æ•°
def mean_pooling(output, mask):
    embeddings = output.last_hidden_state
    mask_expanded = mask.unsqueeze(-1).expand(embeddings.size()).float()
    return torch.sum(embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# æµ‹è¯•æœ¯è¯­å¯¹
term_pairs = [
    ("Primary cultured cells", "primary cultured cell"),
    ("Primary cultured cells", "neural crest derived fibroblast"),
    ("T cell", "B cell"),
    ("neuron", "glial cell"),
    ("macrophage", "dendritic cell"),
    ("erythrocyte", "keratinocyte")
]

# è®¡ç®—ç›¸ä¼¼åº¦
def compute_similarity(model, tokenizer, text_pair):
    inputs = tokenizer(text_pair, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = mean_pooling(outputs, inputs['attention_mask'])
    return F.cosine_similarity(embeddings[0], embeddings[1], dim=0).item()

# ç»“æœæ”¶é›†
results = []
for pair in term_pairs:
    base_sim = compute_similarity(base_model, base_tokenizer, pair)
    ft_sim = compute_similarity(finetuned_model, base_tokenizer, pair)  # ä½¿ç”¨å®Œæ•´å¾®è°ƒæ¨¡å‹
    
    results.append({
        "term_pair": pair,
        "base_model": base_sim,
        "finetuned_model": ft_sim,
        "delta": ft_sim - base_sim
    })

# æ‰“å°ç»“æœ
print("\n{:<50} {:<15} {:<15} {:<10}".format("Term Pair", "Base Model", "Finetuned", "Delta"))
for res in results:
    print("{:<50} {:<15.4f} {:<15.4f} {:<10.4f}".format(
        f"{res['term_pair'][0]} vs {res['term_pair'][1]}",
        res['base_model'],
        res['finetuned_model'],
        res['delta']
    ))

# å¯è§†åŒ–
labels = [f"{p[0]}\nvs\n{p[1]}" for p in term_pairs]
base_sims = [r['base_model'] for r in results]
ft_sims = [r['finetuned_model'] for r in results]

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
rects1 = ax.bar(x - width/2, base_sims, width, label='Base Model')
rects2 = ax.bar(x + width/2, ft_sims, width, label='Finetuned Model')

ax.set_ylabel('Cosine Similarity')
ax.set_title('Embedding Similarity Comparison (Corrected)')
ax.set_xticks(x)
ax.set_xticklabels(labels, rotation=45, ha="right")
ax.legend()

fig.tight_layout()
# plt.savefig("similarity_comparison_corrected.png", bbox_inches='tight', dpi=300)
plt.show()