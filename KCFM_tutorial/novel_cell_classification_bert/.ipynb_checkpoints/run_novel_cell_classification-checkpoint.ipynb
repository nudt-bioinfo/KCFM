{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74724ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the root directory of the project\n",
    "project_root = Path(\"/home/lxz/scmamba/KCellFM_tutorial/novel_cell_classification_bert\").parent.parent\n",
    "# project_root = Path(__file__).parent.parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16562375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxz/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import rankdata\n",
    "from transformers import AutoModel, AutoTokenizer, BertModel, BertConfig\n",
    "from models.model import MambaModel\n",
    "from models.gene_tokenizer import GeneVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfafb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "class Config:\n",
    "    # Data path\n",
    "    pretrain_subset_path = \"/mnt/HHD16T/DATA/lxz/sctab/merlin_cxg_2023_05_15_sf-log1p/novel_cell_clssification/train_data_subset_0_01.h5ad\"\n",
    "    var_parquet_path = \"/mnt/HHD16T/DATA/lxz/sctab/merlin_cxg_2023_05_15_sf-log1p/var.parquet\"\n",
    "    cell_type_parquet_path = \"/mnt/HHD16T/DATA/lxz/sctab/merlin_cxg_2023_05_15_sf-log1p/categorical_lookup/cell_type.parquet\"\n",
    "    converted_data_dir = Path(\n",
    "        \"/mnt/HHD16T/DATA/lxz/sctab/merlin_cxg_2023_05_15_sf-log1p/novel_cell_clssification/scCello_ood_celltype_data1/filtered_data_10_percent\")\n",
    "    json_relationship_path = Path(\"../data/celltype_relationship.json\")\n",
    "\n",
    "    triples_ontology_path = \"/home/lxz/scmamba/novel_cell_classification_bert/data/triples.txt\"\n",
    "\n",
    "    # Output path\n",
    "    ontology_graph_path = \"../data/cell_ontology_graph.json\"\n",
    "    id_to_node_path = \"../data/cell_id_to_node_id_mapping.json\"\n",
    "    id_to_name_path = \"../data/cell_id_to_cell_name_mapping.json\"\n",
    "    cell_type_repr_path = \"../data/cell_type_representations_mamba.json\"\n",
    "    novel_cell_emb_path = \"/mnt/HHD16T/DATA/lxz/sctab/merlin_cxg_2023_05_15_sf-log1p/novel_cell_clssification/data_hard_disk/mamba_bert/novel_cell_embeddings_mamba_bert.pkl\"\n",
    "    cell_type_label_path = \"/home/lxz/scmamba/novel_cell_classification_bert/data/novel_cell_name_to_label_mamba_bert.json\"\n",
    "    results_output_path = \"/home/lxz/scmamba/novel_cell_classification_bert/results/novel_cell_classification_results_mamba_bert.csv\"\n",
    "\n",
    "    # Pre-trained model configuration\n",
    "    pretrained_model_path = \"/home/lxz/scmamba/model_state/cell_cls_3loss_6layer_final.pth\"\n",
    "    gene_vocab_path = \"/home/lxz/scmamba/vocab.json\"\n",
    "    ensembl_ID_to_gene_name_path = \"/home/lxz/scmamba/novel_cell_classification/data/ensembl_ID_to_gene_name_dict_gc30M.pkl\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(gene_vocab_path)\n",
    "\n",
    "    # PubMedbert model configuration\n",
    "    pretrained_bert_model_path = \"/home/lxz/PubMedbert\"\n",
    "    pretrained_bert_checkpoint_path = \"/home/lxz/PubMedbert/finetune_bert.pth\"\n",
    "\n",
    "    # Pre-trained model parameters\n",
    "    max_seq_len = 4096  # Maximum length of sequence\n",
    "    ntoken = len(vocab)  # Vocabulary size, to be loaded from the vocabulary later\n",
    "    embsize = 512  # Embedding dimension\n",
    "    nhead = 8  # Attention head count\n",
    "    d_hid = 512  # Hidden layer dimension\n",
    "    nlayers = 6  # layers\n",
    "    dropout = 0.1  # dropout\n",
    "    pad_token = \"<pad>\"  # <pad> token\n",
    "    pad_value = -2  # <pad> value\n",
    "    input_emb_style = \"continuous\"  # Input embedding style\n",
    "    cell_emb_style = \"cls\"  # Cell embedding style\n",
    "    class_num = 164  # Number of categories\n",
    "\n",
    "    # Device setting\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64\n",
    "    num_workers = 8\n",
    "\n",
    "    # PPR parameters\n",
    "    alpha = 0.9\n",
    "    threshold = 1e-4\n",
    "\n",
    "    # Difficulty grading parameters\n",
    "    difficulty_ratios = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "    selected_ratios = difficulty_ratios \n",
    "    num_samplings = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b05fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_is_a_cells(triplet_path):\n",
    "    \"\"\"Extract all cell types of is_a relationships from triplet.txt\"\"\"\n",
    "    is_a_cells = set()\n",
    "    try:\n",
    "        with open(triplet_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                if ' is_a ' in line:\n",
    "                    cell1, cell2 = line.split(' is_a ', 1)\n",
    "                    cell1 = cell1.strip()\n",
    "                    cell2 = cell2.strip()\n",
    "\n",
    "                    is_a_cells.add(cell1)\n",
    "                    is_a_cells.add(cell2)\n",
    "                else:\n",
    "                    continue  # Skip rows with non is_a relationships\n",
    "\n",
    "        print(f\"Successfully extracted{len(is_a_cells)}unique is'a related cell types\")\n",
    "        return is_a_cells\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Triplet.txt file not found（Path：{triplet_path}）\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: processing triplet.txt：{str(e)}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e007ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pad_token_id = config.vocab[config.pad_token]\n",
    "\n",
    "        # Load gene vocabulary list\n",
    "        self.vocab = GeneVocab.from_file(config.gene_vocab_path)\n",
    "        config.ntoken = len(self.vocab)  # Dynamically set vocabulary size\n",
    "\n",
    "        # Initialize pre-trained model\n",
    "        self.model = MambaModel(\n",
    "            ntoken=config.ntoken,\n",
    "            embsize=config.embsize,\n",
    "            nhead=config.nhead,\n",
    "            d_hid=config.d_hid,\n",
    "            nlayers=config.nlayers,\n",
    "            dropout=config.dropout,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            input_emb_style=config.input_emb_style,\n",
    "            cell_emb_style=config.cell_emb_style\n",
    "        )\n",
    "\n",
    "        # Load pre-trained weights\n",
    "        self._load_pretrained_weights(config.pretrained_model_path, config.device)\n",
    "\n",
    "        # Do not fine tune the model, freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pad_token_id = self.vocab[self.vocab.pad_token] if self.vocab.pad_token else 0\n",
    "        self.cls_token_id = self.vocab[\"<cls>\"] if \"<cls>\" in self.vocab else 1\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "\n",
    "    def _load_pretrained_weights(self, model_path, device):\n",
    "        try:\n",
    "            pretrained_dict = torch.load(model_path, map_location=device)\n",
    "            model_dict = self.model.state_dict()\n",
    "\n",
    "            # Filter out unmatched weights\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                               if k in model_dict and v.shape == model_dict[k].shape}\n",
    "\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.model.load_state_dict(model_dict)\n",
    "            print(\"Successfully loaded pre-trained weights\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load pre-trained weights: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask=None):\n",
    "        src_key_padding_mask = (input_ids == self.pad_token_id) if attention_mask is None else attention_mask.bool()\n",
    "        output = self.model(src=input_ids, values=values, src_key_padding_mask=src_key_padding_mask)\n",
    "        return output[\"cell_emb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "730015bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class finetune_BERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0)\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        return self.projection(pooled)\n",
    "\n",
    "\n",
    "# 4. PubMedBERT embedding extractor\n",
    "class BertEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.pretrained_bert_model_path)\n",
    "        self.finetuned_model = finetune_BERT(\n",
    "            AutoModel.from_pretrained(config.pretrained_bert_model_path)\n",
    "        ).to(config.device)\n",
    "\n",
    "        # Load pre training weights and set them as evaluation mode\n",
    "        self.checkpoint = torch.load(config.pretrained_bert_checkpoint_path, map_location=config.device)\n",
    "        self.finetuned_model.bert.load_state_dict(self.checkpoint['bert_state'])\n",
    "        self.finetuned_model.projection.load_state_dict(self.checkpoint['projection_state'])\n",
    "        self.finetuned_model.eval()\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        model = self.finetuned_model\n",
    "        tokenizer = self.tokenizer\n",
    "        device = self.config.device\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=25).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        return output.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9108afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NovelCellDataset(Dataset):\n",
    "    def __init__(self, parquet_files, ensembl_to_gene_name_dict):\n",
    "        self.parquet_files = parquet_files\n",
    "        self.ensembl_to_gene_name_dict = ensembl_to_gene_name_dict\n",
    "        self.data = []\n",
    "\n",
    "        # vocab\n",
    "        self.vocab = GeneVocab.from_file(Config.gene_vocab_path)\n",
    "        self.pad_token_id = self.vocab[self.vocab.pad_token] if self.vocab.pad_token else 0\n",
    "        self.cls_token_id = self.vocab[\"<cls>\"] if \"<cls>\" in self.vocab else 1\n",
    "\n",
    "        # max_seq_length\n",
    "        self.max_seq_length = Config.max_seq_len\n",
    "\n",
    "        # Preloading all data\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all .parquet file data and filter out zero expression values\"\"\"\n",
    "        for file in tqdm(self.parquet_files, desc=\"加载新细胞数据\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(file)\n",
    "                for _, row in df.iterrows():\n",
    "                    # Handling illegal data: deleting the first element\n",
    "                    expr_nums = row['gene_expression_nums'][1:]\n",
    "                    gene_names = row['gene_ensembl_ids'][1:]\n",
    "\n",
    "                    # Convert to numpy array for easy filtering\n",
    "                    expr_array = np.array(expr_nums)\n",
    "                    gene_array = np.array(gene_names, dtype=object)\n",
    "\n",
    "                    # Mask for obtaining non-zero expression values (>0)\n",
    "                    nonzero_mask = expr_array > 0\n",
    "                    # Filter to obtain non-zero expression values and their corresponding gene names\n",
    "                    nonzero_expr = expr_array[nonzero_mask]\n",
    "                    nonzero_genes = gene_array[nonzero_mask]\n",
    "\n",
    "                    # Map token\n",
    "                    valid_tokens = []\n",
    "                    valid_expr = []\n",
    "                    for gene, expr in zip(nonzero_genes, nonzero_expr):\n",
    "                        if gene is not None:\n",
    "                            if gene.startswith(\"ENSG\"):\n",
    "                                gene_name = self.ensembl_to_gene_name_dict.get(gene, None)\n",
    "                                if gene_name is not None and gene_name in self.vocab:\n",
    "                                    valid_tokens.append(self.vocab[gene_name])\n",
    "                                    valid_expr.append(expr)\n",
    "                            else:\n",
    "                                gene_name = gene\n",
    "                                if gene_name is not None and gene_name in self.vocab:\n",
    "                                    valid_tokens.append(self.vocab[gene_name])\n",
    "                                    valid_expr.append(expr)\n",
    "\n",
    "                    if valid_tokens:\n",
    "                        # Add CLS token\n",
    "                        unsorted_tokens = [self.cls_token_id] + list(valid_tokens)\n",
    "                        unsorted_expr = [0.0] + list(valid_expr)\n",
    "\n",
    "                        self.data.append({\n",
    "                            \"expr\": unsorted_expr,\n",
    "                            \"tokens\": unsorted_tokens,\n",
    "                            \"cell_type\": row['cell_type']\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error: loading file {file.name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        expr = item[\"expr\"]\n",
    "        cell_type = item[\"cell_type\"]\n",
    "\n",
    "        # Truncate or fill to maximum length\n",
    "        if len(tokens) >= self.max_seq_length:\n",
    "            input_ids = tokens[:self.max_seq_length]\n",
    "            values = expr[:self.max_seq_length]\n",
    "            attention_mask = [0] * self.max_seq_length  # 0 indicates validity (consistent with the src_key_madding.mask of the pre-trained model)\n",
    "        else:\n",
    "            padding_len = self.max_seq_length - len(tokens)\n",
    "            input_ids = tokens + [self.pad_token_id] * padding_len\n",
    "            values = expr + [Config.pad_value] * padding_len\n",
    "            attention_mask = [0] * len(tokens) + [1] * padding_len\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"values\": torch.tensor(values, dtype=torch.float32),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"cell_type\": cell_type\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4bf799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel_cell_embeddings():\n",
    "    \"\"\"Generate embedded representations for new cells\"\"\"\n",
    "    if os.path.exists(Config.novel_cell_emb_path) and os.path.exists(Config.cell_type_label_path):\n",
    "        print(\"New cell embedding and label mapping already exist, load directly...\")\n",
    "        with open(Config.novel_cell_emb_path, 'rb') as f:\n",
    "            novel_cell_data = pickle.load(f)\n",
    "        with open(Config.cell_type_label_path, 'r') as f:\n",
    "            cell_type_to_label = json.load(f)\n",
    "        return novel_cell_data, cell_type_to_label\n",
    "\n",
    "    print(\"Start generating new cell embeddings...\")\n",
    "    # Generate mapping from cell types to numerical labels\n",
    "    cell_types = set()\n",
    "    parquet_files = sorted(Config.converted_data_dir.glob(\"*.parquet\"))\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"Error: Parquet file not found in{Config.converted_data_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    for file in tqdm(parquet_files, desc=\"Collect novel cell types\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            current_types = set(df['cell_type'].unique())\n",
    "            cell_types.update(current_types)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: processing file {file.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Sort and generate label mappings\n",
    "    sorted_cell_types = sorted(cell_types)\n",
    "    cell_type_to_label = {cell_type: idx for idx, cell_type in enumerate(sorted_cell_types)}\n",
    "\n",
    "    # Save label mapping\n",
    "    with open(Config.cell_type_label_path, 'w') as f:\n",
    "        json.dump(cell_type_to_label, f, indent=2)\n",
    "\n",
    "    try:\n",
    "        with open(Config.ensembl_ID_to_gene_name_path, 'rb') as f:\n",
    "            ensembl_to_gene_name_dict = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read{Config.ensembl_ID_to_gene_name_path}：{str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "    # Create dataset and dataset loader\n",
    "    dataset = NovelCellDataset(parquet_files, ensembl_to_gene_name_dict)\n",
    "    print(f\"The novel cell dataset has been loaded and contains {len(dataset)} valid cells\")\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # load pre-trained model\n",
    "    model = MambaEmbeddingExtractor(Config).to(Config.device)\n",
    "    model.eval()\n",
    "\n",
    "    # Extract embedding\n",
    "    novel_cell_data = []  # Save (embedding, cell_type, label)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extract novel cell embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(Config.device)\n",
    "            values = batch[\"values\"].to(Config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(Config.device)\n",
    "            cell_types = batch[\"cell_type\"]\n",
    "\n",
    "            with autocast():\n",
    "                embeddings = model(input_ids, values, attention_mask=attention_mask)\n",
    "\n",
    "            embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "            # Save embeddings and corresponding cell types\n",
    "            for emb, cell_type in zip(embeddings_np, cell_types):\n",
    "                label = cell_type_to_label[cell_type]\n",
    "                novel_cell_data.append((emb, cell_type, label))\n",
    "\n",
    "    # Save novel cell embeddings\n",
    "    with open(Config.novel_cell_emb_path, 'wb') as f:\n",
    "        pickle.dump(novel_cell_data, f)\n",
    "\n",
    "    print(f\"Novel cell embedding has been saved to {Config.novel_cell_emb_path}\")\n",
    "    return novel_cell_data, cell_type_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e88996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel_cell_type_embeddings():\n",
    "    if os.path.exists(Config.cell_type_label_path):\n",
    "        print(\"The novel cell type already exists, load directly...\")\n",
    "        with open(Config.cell_type_label_path, 'r') as f:\n",
    "            novel_cell_type_to_label = json.load(f)\n",
    "    else:\n",
    "        print(\"Novel cell type does not exist, program exits\")\n",
    "        exit()\n",
    "\n",
    "    bertEmbeddingExtractor = BertEmbeddingExtractor(Config)\n",
    "\n",
    "    novel_cell_type_emb = []\n",
    "    novel_cell_type_emb_dict = {}\n",
    "    for cell_type in tqdm(novel_cell_type_to_label.keys(), desc=\"Generate novel cell type embeddings\"):\n",
    "        emb = bertEmbeddingExtractor.get_embedding(cell_type)\n",
    "        novel_cell_type_emb.append(emb.squeeze(0))\n",
    "        novel_cell_type_emb_dict[cell_type] = emb.squeeze(0)\n",
    "\n",
    "    novel_cell_type_emb = np.array(novel_cell_type_emb)\n",
    "    return novel_cell_type_emb_dict, novel_cell_type_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2596f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ontology_cell_type_emb():\n",
    "    if os.path.exists(Config.triples_ontology_path):\n",
    "        print(\"The knowledge graph cell type already exists, load directly...\")\n",
    "        ontology_cell_type_unique = extract_is_a_cells(Config.triples_ontology_path)\n",
    "    else:\n",
    "        print(\"Knowledge graph cell type does not exist, program exits\")\n",
    "        exit()\n",
    "\n",
    "    bertEmbeddingExtractor = BertEmbeddingExtractor(Config)\n",
    "\n",
    "    ontology_cell_type_emb = []\n",
    "    ontology_cell_type_emb_dict = {}\n",
    "    for cell_type in tqdm(ontology_cell_type_unique, desc=\"Emb of cell types in generating knowledge graphs\"):\n",
    "        emb = bertEmbeddingExtractor.get_embedding(cell_type)\n",
    "        ontology_cell_type_emb.append(emb.squeeze(0))\n",
    "        ontology_cell_type_emb_dict[cell_type] = emb.squeeze(0)\n",
    "    ontology_cell_type_emb = np.array(ontology_cell_type_emb)\n",
    "    return ontology_cell_type_emb_dict, ontology_cell_type_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db48be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_cell_name_to_lower_cell_name_mapping(cell_name_list):\n",
    "    \"\"\"Convert cell type names to lowercase mapping dictionary\"\"\"\n",
    "    cell_name_lower_dict = {}\n",
    "    for name in cell_name_list:\n",
    "        new_name = name.lower()\n",
    "        cell_name_lower_dict[name] = new_name\n",
    "    return cell_name_lower_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "074e98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_novel_cells(novel_cell_data, ontology_cell_type_emb_dict, novel_cell_type_to_label):\n",
    "    # Prepare data\n",
    "    cell_embeddings = np.array([item[0] for item in novel_cell_data])  # Cell emb\n",
    "    true_labels = np.array([item[2] for item in novel_cell_data])  # True label\n",
    "    true_cell_types = [item[1] for item in novel_cell_data]  # Real cell type name\n",
    "\n",
    "    # Retrieve the cell type (in lowercase) from novel_cell_type_to_label\n",
    "    novel_types = set(novel_cell_type_to_label.keys())\n",
    "    novel_types_lower = {t.lower() for t in novel_types}\n",
    "\n",
    "    # Select the types that exist in novel_cell_type_to_label from ontology\n",
    "    filtered_ontology_types = []\n",
    "    filtered_ontology_embs = []\n",
    "\n",
    "    # Create ontology type to lowercase mapping\n",
    "    all_ontology_types = list(ontology_cell_type_emb_dict.keys())\n",
    "    ontology_type_lower = get_upper_cell_name_to_lower_cell_name_mapping(all_ontology_types)\n",
    "\n",
    "    # screening process\n",
    "    for ont_type in all_ontology_types:\n",
    "        ont_type_lower = ontology_type_lower[ont_type]\n",
    "        if ont_type_lower in novel_types_lower:\n",
    "            filtered_ontology_types.append(ont_type)\n",
    "            filtered_ontology_embs.append(ontology_cell_type_emb_dict[ont_type])\n",
    "\n",
    "    # Check the screening results\n",
    "    print(f\"Select {len(filtered_ontology_types)} types from ontology that exist in novel cell data\")\n",
    "    if len(filtered_ontology_types) == 0:\n",
    "        print(\"Error: No overlapping cell types found, unable to classify\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Convert to numpy array\n",
    "    filtered_ontology_embs = np.array(filtered_ontology_embs)\n",
    "\n",
    "    # Create filtered type to lowercase mapping and label mapping\n",
    "    filtered_type_lower = get_upper_cell_name_to_lower_cell_name_mapping(filtered_ontology_types)\n",
    "    novel_type_lower = {k.lower(): v for k, v in novel_cell_type_to_label.items()}\n",
    "\n",
    "    # Calculate the cosine similarity between all cells and the selected type embeddings\n",
    "    print(\"Calculate the cosine similarity between cell embeddings and filtered type embeddings...\")\n",
    "    similarities = cosine_similarity(cell_embeddings, filtered_ontology_embs)  # Shape: [n_cells, n_filtered_types]\n",
    "\n",
    "    ########################################\n",
    "    # \"\"\"Using Spearman to measure the correlation between vectors\"\"\"\n",
    "    # # Convert data to rank\n",
    "    # cell_ranks = rankdata(cell_embeddings, axis=1)\n",
    "    # ontology_ranks = rankdata(filtered_ontology_embs, axis=1)\n",
    "    #\n",
    "    # # Calculate correlation (1-rank distance)\n",
    "    # # Note: Correlation is used here instead of distance, so subtract 1 from it\n",
    "    # similarities = 1 - cdist(cell_ranks, ontology_ranks, metric='correlation')\n",
    "    ########################################\n",
    "\n",
    "    # Predict the type of each cell\n",
    "    pred_labels = []\n",
    "    for i in range(len(cell_embeddings)):\n",
    "        # Find the most similar screened type\n",
    "        max_sim_idx = np.argmax(similarities[i])\n",
    "        pred_type = filtered_ontology_types[max_sim_idx]\n",
    "        pred_type_lower = filtered_type_lower[pred_type]\n",
    "\n",
    "        # Obtain the corresponding label\n",
    "        pred_label = novel_type_lower[pred_type_lower]\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "\n",
    "    return true_labels, true_cell_types, pred_labels, similarities, filtered_ontology_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde91c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_difficulty_levels(true_labels, true_cell_types, pred_labels, similarities,\n",
    "                                    filtered_ontology_types, novel_cell_type_to_label, selected_ratios=None):\n",
    "    if selected_ratios is None:\n",
    "        selected_ratios = Config.selected_ratios\n",
    "\n",
    "    novel_type_lower = {k.lower(): k for k in novel_cell_type_to_label.keys()}\n",
    "    \n",
    "    filtered_to_novel_type = {}\n",
    "    for ont_type in filtered_ontology_types:\n",
    "        ont_type_lower = ont_type.lower()\n",
    "        if ont_type_lower in novel_type_lower:\n",
    "            filtered_to_novel_type[ont_type] = novel_type_lower[ont_type_lower]\n",
    "\n",
    "    n_filtered_types = len(filtered_ontology_types)\n",
    "    all_result, avg_result = [], []\n",
    "\n",
    "    # Create a mapping from labels to cell types\n",
    "    label_to_type = {v: k for k, v in novel_cell_type_to_label.items()}\n",
    "\n",
    "    # Ensure that the output directory exists\n",
    "    os.makedirs(os.path.dirname(Config.results_output_path), exist_ok=True)\n",
    "\n",
    "    for ratio in selected_ratios:\n",
    "        # Calculate the number of types to be selected this time\n",
    "        num_selected = int(n_filtered_types * ratio + 0.5)\n",
    "        num_selected = max(1, num_selected)  # Ensure that at least one type is selected\n",
    "\n",
    "        print(f\"\\nAssess difficulty level: {ratio} (Select {num_selected}/{n_filtered_types} ontology overlap types)\")\n",
    "\n",
    "        all_acc, all_f1 = [], []\n",
    "        num_samplings = 1 if ratio >= 1.0 else Config.num_samplings\n",
    "\n",
    "        for _ in range(num_samplings):\n",
    "            selected_filtered_indices = np.random.permutation(n_filtered_types)[:num_selected]\n",
    "            # Retrieve the selected ontology type\n",
    "            selected_filtered_types = [filtered_ontology_types[i] for i in selected_filtered_indices]\n",
    "            # Map to the corresponding new cell type\n",
    "            selected_novel_types = [filtered_to_novel_type[t] for t in selected_filtered_types]\n",
    "            # Obtain the original label corresponding to the new cell type\n",
    "            selected_novel_labels = [novel_cell_type_to_label[t] for t in selected_novel_types]\n",
    "\n",
    "            # Create a new label mapping (renumber within the selected type)\n",
    "            new_label_mapping = {old_label: idx for idx, old_label in enumerate(selected_novel_labels)}\n",
    "\n",
    "            # Filter out cells belonging to the selected type\n",
    "            used_indices = np.isin(true_labels, selected_novel_labels)\n",
    "\n",
    "            if np.sum(used_indices) == 0:\n",
    "                print(\"Warning: No cells of the selected type were found, skip this sampling\")\n",
    "                continue\n",
    "\n",
    "            # Remap real labels\n",
    "            new_true_labels = [new_label_mapping[x] for x in np.array(true_labels)[used_indices]]\n",
    "\n",
    "            selected_similarities = similarities[used_indices][:, selected_filtered_indices]\n",
    "            new_preds = np.argmax(selected_similarities, axis=1)\n",
    "\n",
    "            # calculated metrics\n",
    "            acc = accuracy_score(new_true_labels, new_preds)\n",
    "            f1 = f1_score(new_true_labels, new_preds, average=\"macro\")\n",
    "\n",
    "            all_acc.append(acc)\n",
    "            all_f1.append(f1)\n",
    "\n",
    "            # For a complete dataset, output a detailed report\n",
    "            if ratio >= 1.0:\n",
    "                print(\"\\nComplete classification report:\")\n",
    "                print(classification_report(\n",
    "                    new_true_labels,\n",
    "                    new_preds,\n",
    "                    target_names=selected_novel_types,\n",
    "                    digits=4,\n",
    "                    zero_division=0\n",
    "                ))\n",
    "                break\n",
    "\n",
    "        # calculate the average\n",
    "        divnum = len(all_acc) if len(all_acc) > 0 else 1\n",
    "        avg_acc = round(sum(all_acc) / divnum, 4) if divnum > 0 else 0\n",
    "        avg_f1 = round(sum(all_f1) / divnum, 4) if divnum > 0 else 0\n",
    "\n",
    "        avg_result.append((ratio, avg_acc, avg_f1))\n",
    "        all_result.append((ratio, all_acc, all_f1))\n",
    "\n",
    "        print(f\"Difficulty level {ratio} -  Average accuracy: {avg_acc}, Average F1 score: {avg_f1}\")\n",
    "\n",
    "    # Save results\n",
    "    avg_result_df = pd.DataFrame(avg_result, columns=[\"ratio\", \"acc\", \"f1\"])\n",
    "    avg_result_df.to_csv(Config.results_output_path, index=False)\n",
    "    print(f\"\\nThe evaluation results have been saved to {Config.results_output_path}\")\n",
    "\n",
    "    return avg_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be344c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: Extract cell types from the knowledge graph\n",
    "    triples_cell_type_unique = extract_is_a_cells(Config.triples_ontology_path)\n",
    "    if not triples_cell_type_unique:\n",
    "        print(\"Unable to continue with subsequent tasks as the cell types related to is_a were not extracted\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Generate new cell embeddings\n",
    "    novel_cell_data, cell_type_to_label = generate_novel_cell_embeddings()\n",
    "    if novel_cell_data is None:\n",
    "        print(\"Unable to generate novel cell embeddings, program exits\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Use BERT model to generate embeddings for new cell types\n",
    "    print(\"Generate BERT embeddings for novel cell types...\")\n",
    "    _, _ = generate_novel_cell_type_embeddings()\n",
    "\n",
    "    # Step 4: Use BERT model to generate embeddings for cell types in the knowledge graph\n",
    "    print(\"BERT embedding for generating knowledge graph cell types...\")\n",
    "    ontology_cell_type_emb_dict, _ = generate_ontology_cell_type_emb()\n",
    "\n",
    "    # Step 5: Classify novel cell data\n",
    "    print(\"Start classifying novel cells...\")\n",
    "    true_labels, true_cell_types, pred_labels, similarities, filtered_ontology_types = classify_novel_cells(\n",
    "        novel_cell_data, ontology_cell_type_emb_dict, cell_type_to_label)\n",
    "\n",
    "    if true_labels is None:\n",
    "        print(\"Classification failed, unable to conduct difficulty level assessment\")\n",
    "        return\n",
    "\n",
    "    # Step 6: Evaluate by difficulty level\n",
    "    evaluate_with_difficulty_levels(\n",
    "        true_labels,\n",
    "        true_cell_types,\n",
    "        pred_labels,\n",
    "        similarities,\n",
    "        filtered_ontology_types,\n",
    "        cell_type_to_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c170d98f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted2686unique is'a related cell types\n",
      "New cell embedding and label mapping already exist, load directly...\n",
      "Generate BERT embeddings for novel cell types...\n",
      "The novel cell type already exists, load directly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate novel cell type embeddings: 100%|██████████████████████████████████████████████| 75/75 [00:04<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embedding for generating knowledge graph cell types...\n",
      "The knowledge graph cell type already exists, load directly...\n",
      "Successfully extracted2686unique is'a related cell types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emb of cell types in generating knowledge graphs: 100%|████████████████████████████| 2686/2686 [00:14<00:00, 180.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start classifying novel cells...\n",
      "Select 72 types from ontology that exist in novel cell data\n",
      "Calculate the cosine similarity between cell embeddings and filtered type embeddings...\n",
      "\n",
      "Assess difficulty level: 0.1 (Select 7/72 ontology overlap types)\n",
      "Difficulty level 0.1 -  Average accuracy: 0.6932, Average F1 score: 0.5631\n",
      "\n",
      "Assess difficulty level: 0.25 (Select 18/72 ontology overlap types)\n",
      "Difficulty level 0.25 -  Average accuracy: 0.5406, Average F1 score: 0.359\n",
      "\n",
      "Assess difficulty level: 0.5 (Select 36/72 ontology overlap types)\n",
      "Difficulty level 0.5 -  Average accuracy: 0.4251, Average F1 score: 0.2498\n",
      "\n",
      "Assess difficulty level: 0.75 (Select 54/72 ontology overlap types)\n",
      "Difficulty level 0.75 -  Average accuracy: 0.3767, Average F1 score: 0.1943\n",
      "\n",
      "Assess difficulty level: 1.0 (Select 72/72 ontology overlap types)\n",
      "\n",
      "Complete classification report:\n",
      "                                                                                      precision    recall  f1-score   support\n",
      "\n",
      "                          CD8-alpha-beta-positive, alpha-beta intraepithelial T cell     0.4943    0.8208    0.6170       106\n",
      "                                       intestinal crypt stem cell of small intestine     0.0000    0.0000    0.0000        94\n",
      "                                                                 fetal cardiomyocyte     1.0000    0.0076    0.0152       131\n",
      "                                                                  brown preadipocyte     0.0000    0.0000    0.0000       572\n",
      "                                                                  tuft cell of colon     0.0000    0.0000    0.0000        52\n",
      "                                                         type D enteroendocrine cell     0.0000    0.0000    0.0000         5\n",
      "                                                          intrahepatic cholangiocyte     0.0000    0.0000    0.0000         3\n",
      "                                                        epithelial cell of esophagus     0.6481    0.1520    0.2462      2836\n",
      "                                                                            GIP cell     0.0000    0.0000    0.0000         6\n",
      "                                                       enteroendocrine cell of colon     0.0255    0.6842    0.0491        19\n",
      "                                                                    endocardial cell     0.0333    0.0060    0.0102       166\n",
      "                                                                lens epithelial cell     0.0000    0.0000    0.0000       527\n",
      "                                                                          eosinophil     0.0667    0.0244    0.0357        41\n",
      "                                                 group 2 innate lymphoid cell, human     0.0175    0.4444    0.0338         9\n",
      "                                                                  Cajal-Retzius cell     0.0000    0.0000    0.0000        41\n",
      "                                                                      reticular cell     0.0000    0.0000    0.0000       189\n",
      "                                                     ovarian surface epithelial cell     0.6762    1.0000    0.8068       808\n",
      "                                                                        Kupffer cell     0.6316    0.0169    0.0330       708\n",
      "                                                                          pro-T cell     0.0000    0.0000    0.0000      1218\n",
      "                                                                 large pre-B-II cell     0.0000    0.0000    0.0000        87\n",
      "                      L5 extratelencephalic projecting glutamatergic cortical neuron     1.0000    0.9955    0.9977       222\n",
      "                                                                    OFF-bipolar cell     0.0000    0.0000    0.0000      1628\n",
      "                                                               DN1 thymic pro-T cell     0.0000    0.0000    0.0000        98\n",
      "                                     endothelial cell of periportal hepatic sinusoid     0.0000    0.0000    0.0000        15\n",
      "                                                                       DN4 thymocyte     0.0000    0.0000    0.0000         3\n",
      "                                                                paneth cell of colon     0.0000    0.0000    0.0000       163\n",
      "                                                              mesothelial fibroblast     0.0038    0.5217    0.0075        23\n",
      "                                                                     lens fiber cell     0.0000    0.0000    0.0000        78\n",
      "                                                           fraction A pre-pro B cell     0.6423    0.3823    0.4793       667\n",
      "                                                              kidney epithelial cell     0.8225    0.5338    0.6475      1094\n",
      "                                                                         interneuron     0.0121    0.7097    0.0239        31\n",
      "                                                                          centrocyte     0.0000    0.0000    0.0000        59\n",
      "                                                                   ileal goblet cell     0.0294    1.0000    0.0572        23\n",
      "                                                         type N enteroendocrine cell     0.0000    0.0000    0.0000         1\n",
      "                                                                 immature neutrophil     0.2000    0.0455    0.0741       132\n",
      "                                           hematopoietic multipotent progenitor cell     0.7220    0.8121    0.7644       777\n",
      "                                                                    colon macrophage     0.0208    0.4231    0.0397        26\n",
      "                                                             corneal epithelial cell     0.4578    0.4519    0.4548       540\n",
      "                                                        tracheobronchial goblet cell     0.3508    0.7436    0.4767       117\n",
      "                                               serous cell of epithelium of bronchus     0.0000    0.0000    0.0000         1\n",
      "                                                        renal beta-intercalated cell     0.6154    0.5217    0.5647        92\n",
      "                                                                     Langerhans cell     0.0000    0.0000    0.0000        59\n",
      "                                                           bronchial epithelial cell     0.0128    0.1449    0.0235        69\n",
      "                                                    fibro/adipogenic progenitor cell     0.0000    0.0000    0.0000      1602\n",
      "                                               progenitor cell of endocrine pancreas     0.0000    0.0000    0.0000         4\n",
      "                                                          endothelial cell of uterus     0.7224    0.3941    0.5100      1327\n",
      "                                                                      endosteal cell     0.0000    0.0000    0.0000         5\n",
      "                                                              pancreatic acinar cell     0.0000    0.0000    0.0000      2821\n",
      "                                                       myeloid dendritic cell, human     0.0659    1.0000    0.1236        17\n",
      "CD34-positive, CD56-positive, CD117-positive common innate lymphoid precursor, human     0.2143    0.1765    0.1935        34\n",
      "                                                                 ciliary muscle cell     0.0584    0.0853    0.0693       293\n",
      "                                                       basophil mast progenitor cell     0.0893    0.9091    0.1626        22\n",
      "                                                                     myometrial cell     0.0000    0.0000    0.0000        19\n",
      "                                                                 papillary tips cell     0.5500    0.6875    0.6111        32\n",
      "                                                                  pulmonary ionocyte     0.0000    0.0000    0.0000       313\n",
      "                                                         common dendritic progenitor     0.1194    0.0755    0.0925       106\n",
      "                                                           pigmented epithelial cell     0.0000    0.0000    0.0000       200\n",
      "                                                                        prickle cell     0.5723    0.9172    0.7048      2632\n",
      "                                                      smooth muscle cell of prostate     0.1003    0.0902    0.0950       388\n",
      "                                                                    slow muscle cell     0.3627    0.9867    0.5305        75\n",
      "                                                serous cell of epithelium of trachea     0.0189    0.3333    0.0357         3\n",
      "                                                                   follicular B cell     0.7259    0.9869    0.8365      1903\n",
      "                                                      mesothelial cell of epicardium     0.3846    0.1124    0.1739        89\n",
      "                                                  epithelial cell of small intestine     0.0000    0.0000    0.0000        33\n",
      "                                                endothelial cell of hepatic sinusoid     0.0000    0.0000    0.0000        96\n",
      "                                                                            basophil     0.0114    0.0160    0.0133       125\n",
      "                                                  CD4-positive, alpha-beta thymocyte     0.4517    0.3938    0.4208      1996\n",
      "                                                 lung microvascular endothelial cell     0.0246    0.4583    0.0466        48\n",
      "                                                                       cholangiocyte     0.0254    0.0107    0.0150       281\n",
      "                                                                          keratocyte     0.0000    0.0000    0.0000        58\n",
      "                                                             transit amplifying cell     0.0000    0.0000    0.0000      1159\n",
      "                                                              trophoblast giant cell     0.0000    0.0000    0.0000         1\n",
      "\n",
      "                                                                            accuracy                         0.3203     29218\n",
      "                                                                           macro avg     0.1803    0.2510    0.1541     29218\n",
      "                                                                        weighted avg     0.3568    0.3203    0.3006     29218\n",
      "\n",
      "Difficulty level 1.0 -  Average accuracy: 0.3203, Average F1 score: 0.1541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation results have been saved to /home/lxz/scmamba/novel_cell_classification_bert/results/novel_cell_classification_results_mamba_bert.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
