{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deade6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the root directory of the project\n",
    "project_root = Path(\"/home/lxz/scmamba/KCellFM_tutorial/T_cancer_cell\").parent.parent\n",
    "# project_root = Path(__file__).parent.parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e322c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxz/tools/anaconda3/envs/scgpt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from models.model import MambaModel\n",
    "from models.gene_tokenizer import GeneVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6130d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0abb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "embsize = 512\n",
    "nhead = 8\n",
    "d_hid = 512\n",
    "nlayers = 6\n",
    "fine_tune_layers = 2\n",
    "dropout = 0.1\n",
    "lr =2e-4  # 1e-5\n",
    "weight_decay = 1e-3  # 1e-4\n",
    "pad_token = \"<pad>\"\n",
    "max_seq_len = 4096\n",
    "input_emb_style = \"continuous\"\n",
    "cell_emb_style = \"cls\"\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "val_split = 0.3\n",
    "model_save_dir = \"/home/lxz/scmamba/model_state/\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c829c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = GeneVocab.from_file(\"/home/lxz/scmamba/vocab.json\")\n",
    "ntokens = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b347e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_to_id = {\n",
    "    'T cell': 0,\n",
    "    'CD8-positive, alpha-beta cytotoxic T cell': 1,\n",
    "    'naive thymus-derived CD4-positive, alpha-beta T cell': 2,\n",
    "    'effector CD8-positive, alpha-beta T cell': 3,\n",
    "    'effector memory CD8-positive, alpha-beta T cell': 4,\n",
    "    'central memory CD4-positive, alpha-beta T cell': 5,\n",
    "    'gamma-delta T cell': 6\n",
    "}\n",
    "class_num = len(celltype_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52048922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.adata = adata\n",
    "        self.cell_ids = adata.obs_names.tolist()\n",
    "        self.gene_names = adata.var.feature_name.tolist()\n",
    "\n",
    "        self.nonzero_indices = {}\n",
    "        expr_matrix = adata.X.toarray() if sparse.issparse(adata.X) else adata.X\n",
    "        for i, cell_id in enumerate(self.cell_ids):\n",
    "            self.nonzero_indices[cell_id] = np.where(expr_matrix[i] != 0)[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cell_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_id = self.cell_ids[idx]\n",
    "        cell_type = self.adata.obs.loc[cell_id, 'cell_type']\n",
    "\n",
    "        nonzero_idx = self.nonzero_indices[cell_id]\n",
    "        expr_values = self.adata.X[idx, nonzero_idx].toarray().flatten() \\\n",
    "            if sparse.issparse(self.adata.X) \\\n",
    "            else self.adata.X[idx, nonzero_idx]\n",
    "        gene_names = [self.gene_names[i] for i in nonzero_idx]\n",
    "\n",
    "        gene_ids = []\n",
    "        filtered_expr = []\n",
    "        for gene, value in zip(gene_names, expr_values):\n",
    "            if gene in vocab:\n",
    "                gene_ids.append(vocab[gene])\n",
    "                filtered_expr.append(value)\n",
    "\n",
    "        if len(gene_ids) > max_seq_len - 1:  # -1\n",
    "            selected = np.random.choice(len(gene_ids), max_seq_len - 1, replace=False)\n",
    "            gene_ids = [gene_ids[i] for i in selected]\n",
    "            filtered_expr = [filtered_expr[i] for i in selected]\n",
    "\n",
    "        gene_ids = [vocab[\"<cls>\"]] + gene_ids\n",
    "        filtered_expr = [0.0] + filtered_expr  # CLS to 0\n",
    "\n",
    "        padding_len = max_seq_len - len(gene_ids)\n",
    "        if padding_len > 0:\n",
    "            gene_ids += [vocab[\"<pad>\"]] * padding_len\n",
    "            filtered_expr += [pad_value] * padding_len\n",
    "\n",

    "        padding_mask = [id_ == vocab[\"<pad>\"] for id_ in gene_ids]\n",
    "\n",
    "        return {\n",
    "            'src': torch.LongTensor(gene_ids),\n",
    "            'values': torch.FloatTensor(filtered_expr),\n",
    "            'padding_mask': torch.BoolTensor(padding_mask),\n",
    "            'celltype': torch.tensor(celltype_to_id[cell_type], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c28624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            src = batch['src'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            padding_mask = batch['padding_mask'].to(device)\n",
    "            cell_types = batch['celltype'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                model_output = model(\n",
    "                    src=src,\n",
    "                    values=values,\n",
    "                    src_key_padding_mask=padding_mask\n",
    "                )\n",
    "                loss = criterion(model_output[\"cls_output\"], cell_types)\n",
    "                total_loss += loss.item() * src.size(0)\n",
    "\n",
    "                preds = torch.argmax(model_output[\"cls_output\"], dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(cell_types.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "    return avg_loss, accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0084e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Start loading h5ad file...\")\n",
    "    adata_train = sc.read(\"/mnt/HHD16T/DATA/lxz/cancer/T_train.h5ad\")\n",
    "    print(\"H5ad file loading completed, cell count:\", adata_train.n_obs)\n",
    "\n",
    "    print(\"Start building the dataset...\")\n",
    "    full_dataset = SingleCellDataset(adata_train)\n",
    "    print(\"Dataset construction completed, total sample size:\", len(full_dataset))\n",
    "\n",
    "    # Retrieve all labels for stratified sampling\n",
    "    all_labels = [full_dataset[i]['celltype'].item() for i in range(len(full_dataset))]\n",
    "\n",
    "    # Divide the training set and validation set\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(full_dataset)),\n",
    "        test_size=val_split,\n",
    "        stratify=all_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "    print(f\"Training set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create dataset loader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        # pin_memory=True,\n",
    "        # drop_last=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        # pin_memory=True,\n",
    "        # drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MambaModel(\n",
    "        ntokens, embsize, nhead, d_hid, nlayers,\n",
    "        vocab=vocab, dropout=dropout, pad_token=pad_token,\n",
    "        pad_value=pad_value, input_emb_style=input_emb_style,\n",
    "        cell_emb_style=cell_emb_style, class_num=class_num\n",
    "    ).to(device)\n",
    "\n",
    "    # Load pre trained weights (skip classification header)\n",
    "    try:\n",
    "        pretrained_dict = torch.load(\"/home/lxz/scmamba/model_state/cell_cls_3loss_6layer_final.pth\",\n",
    "                                     map_location=device)\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                           if k in model_dict and v.shape == model_dict[k].shape\n",
    "                           and not k.startswith('cls_decoder.out_layer')}\n",
    "\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"Successfully loaded pre training weights (excluding classification head weights)\")\n",
    "\n",
    "        # Reinitialize the classification header\n",
    "        print(\"Initialize classification header weights...\")\n",
    "        nn.init.kaiming_normal_(model.cls_decoder.out_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if model.cls_decoder.out_layer.bias is not None:\n",
    "            nn.init.zeros_(model.cls_decoder.out_layer.bias)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load pre training weights: {str(e)}\")\n",
    "\n",
    "    # Freeze and Unfreeze Logic\n",
    "    print(f\"Freeze the {nlayers - fine_tune_layers} layer before freezing, and only fine tune the final {fine_tune_layers} layer and classifier\")\n",
    "\n",
    "    # Check if the mamba_decoder structure meets expectations\n",
    "    if hasattr(model, 'mamba_encoder') and isinstance(model.mamba_encoder, nn.ModuleList):\n",
    "        # Verify if the number of layers matches the configuration\n",
    "        actual_layers = len(model.mamba_encoder)\n",
    "        if actual_layers != nlayers:\n",
    "            raise ValueError(f\"The number of layers in the mamba_decoder model does not match the configuration. The actual number is {practical_1ayers}, while the expected number is {nlayers}\")\n",
    "\n",
    "        # Freeze the previous layer\n",
    "        for i in range(nlayers - fine_tune_layers):\n",
    "            layer = model.mamba_encoder[i]\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"Freeze layer {i}: Parameters frozen\")\n",
    "\n",
    "        # Unfreeze the last two layers\n",
    "        for i in range(nlayers - fine_tune_layers, nlayers):\n",
    "            layer = model.mamba_encoder[i]\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(f\"Unfreezing layer {i}: parameters trainable\")\n",
    "    else:\n",
    "        raise AttributeError(\"Mamba_decoder not found in the model or it is not of type nn.ModuleList\")\n",
    "\n",
    "    # Ensure that the classifier is trainable\n",
    "    if hasattr(model, 'cls_decoder'):\n",
    "        for param in model.cls_decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"The classifier parameters have been set to trainable\")\n",
    "    else:\n",
    "        raise AttributeError(\"The cls_decoder classifier was not found in the model\")\n",
    "\n",
    "    # Only optimize the parameters that need to be trained\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Number of trainable parameters: {sum(p.numel() for p in trainable_params)}\")\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # training loop\n",
    "    best_val_acc = 0.0\n",
    "    best_model_path = os.path.join(model_save_dir, \"cancer_Tcell_2_layers_best_ipynb.pth\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            src = batch['src'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            padding_mask = batch['padding_mask'].to(device)\n",
    "            cell_types = batch['celltype'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                output = model(src=src, values=values, src_key_padding_mask=padding_mask)\n",
    "                loss = criterion(output[\"cls_output\"], cell_types)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item() * src.size(0)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        # Calculate the average training loss\n",
    "        avg_train_loss = total_train_loss / len(train_dataset)\n",
    "\n",
    "        # Verify\n",
    "        val_loss, val_acc, val_preds, val_labels = evaluate(model, val_loader, device, criterion)\n",
    "\n",
    "        # Print epoch result\n",
    "        print(f\"\\nEpoch {epoch + 1} result:\")\n",
    "        print(f\"Training loss: {avg_train_loss:.4f} | Validation loss: {val_loss:.4f} | Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Save the best mdoel (Validation accuracy: {best_val_acc:.4f}) to {best_model_path}\")\n",
    "\n",
    "    # Print the best results after the training is completed\n",
    "    print(f\"\\nTraining completed! Best Verification Accuracy: {best_val_acc:.4f} (The model is saved in {best_model_path})\")\n",
    "\n",
    "    # Load the best model and print a detailed report of the validation set\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    _, _, val_preds, val_labels = evaluate(model, val_loader, device, criterion)\n",
    "    print(\"\\nDetailed report of the best model on the validation set:\")\n",
    "    print(classification_report(\n",
    "        val_labels,\n",
    "        val_preds,\n",
    "        target_names=celltype_to_id.keys(),\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(model_save_dir, \"cancer_Tcell_2_layers_best_final_ipynb.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"The final model has been saved as {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f708c0f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading h5ad file...\n",
      "H5ad file loading completed, cell count: 30717\n",
      "Start building the dataset...\n",
      "Dataset construction completed, total sample size: 30717\n",
      "Training set size: 21501, Validation set size: 9216\n",
      "Successfully loaded pre training weights (excluding classification head weights)\n",
      "Initialize classification header weights...\n",
      "Freeze the 4 layer before freezing, and only fine tune the final 2 layer and classifier\n",
      "Freeze layer 0: Parameters frozen\n",
      "Freeze layer 1: Parameters frozen\n",
      "Freeze layer 2: Parameters frozen\n",
      "Freeze layer 3: Parameters frozen\n",
      "Unfreezing layer 4: parameters trainable\n",
      "Unfreezing layer 5: parameters trainable\n",
      "The classifier parameters have been set to trainable\n",
      "Number of trainable parameters: 35853512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:31<00:00,  1.49it/s, loss=0.425]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 result:\n",
      "Training loss: 0.5457 | Validation loss: 0.4610 | Validation accuracy: 0.8296\n",
      "Save the best mdoel (Validation accuracy: 0.8296) to /home/lxz/scmamba/model_state/cancer_Tcell_2_layers_best_ipynb.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:13<00:00,  1.55it/s, loss=0.389]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 result:\n",
      "Training loss: 0.3658 | Validation loss: 0.4778 | Validation accuracy: 0.8242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:12<00:00,  1.55it/s, loss=0.0444]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 result:\n",
      "Training loss: 0.2393 | Validation loss: 0.5103 | Validation accuracy: 0.8379\n",
      "Save the best mdoel (Validation accuracy: 0.8379) to /home/lxz/scmamba/model_state/cancer_Tcell_2_layers_best_ipynb.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:11<00:00,  1.56it/s, loss=0.119]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 result:\n",
      "Training loss: 0.1250 | Validation loss: 0.6819 | Validation accuracy: 0.8356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:12<00:00,  1.55it/s, loss=0.0144]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 result:\n",
      "Training loss: 0.0663 | Validation loss: 0.8277 | Validation accuracy: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:13<00:00,  1.55it/s, loss=0.0146]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 result:\n",
      "Training loss: 0.0345 | Validation loss: 1.1379 | Validation accuracy: 0.8279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:25<00:00,  1.51it/s, loss=0.000978]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 result:\n",
      "Training loss: 0.0255 | Validation loss: 1.1546 | Validation accuracy: 0.8338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:09<00:00,  1.56it/s, loss=0.0492]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:00<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 result:\n",
      "Training loss: 0.0211 | Validation loss: 1.2757 | Validation accuracy: 0.8315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:18<00:00,  1.53it/s, loss=0.000494]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:07<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 result:\n",
      "Training loss: 0.0252 | Validation loss: 1.2131 | Validation accuracy: 0.8371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 672/672 [07:14<00:00,  1.54it/s, loss=0.202]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:06<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 result:\n",
      "Training loss: 0.0242 | Validation loss: 1.2684 | Validation accuracy: 0.8256\n",
      "\n",
      "Training completed! Best Verification Accuracy: 0.8379 (The model is saved in /home/lxz/scmamba/model_state/cancer_Tcell_2_layers_best_ipynb.pth)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 288/288 [01:01<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed report of the best model on the validation set:\n",
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                                              T cell     0.9457    0.9812    0.9631      3831\n",
      "           CD8-positive, alpha-beta cytotoxic T cell     0.8194    0.7510    0.7837      1450\n",
      "naive thymus-derived CD4-positive, alpha-beta T cell     0.7758    0.9114    0.8382      1219\n",
      "            effector CD8-positive, alpha-beta T cell     0.7594    0.6542    0.7029      1206\n",
      "     effector memory CD8-positive, alpha-beta T cell     0.6317    0.6359    0.6338       758\n",
      "      central memory CD4-positive, alpha-beta T cell     0.6794    0.4884    0.5683       473\n",
      "                                  gamma-delta T cell     0.7722    0.9355    0.8460       279\n",
      "\n",
      "                                            accuracy                         0.8379      9216\n",
      "                                           macro avg     0.7691    0.7654    0.7623      9216\n",
      "                                        weighted avg     0.8342    0.8379    0.8334      9216\n",
      "\n",
      "The final model has been saved as /home/lxz/scmamba/model_state/cancer_Tcell_2_layers_best_final_ipynb.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad615c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
