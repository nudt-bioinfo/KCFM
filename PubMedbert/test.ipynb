{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 指定本地路径\n",
    "model_path = \"/mnt/c/Users/94903/Desktop/cellannotation/PubMedbert\"\n",
    "\n",
    "# 加载组件\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# 测试推理\n",
    "text1 = \"Primary cultured cells\"\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "# 自定义池化（与原始实现一致）\n",
    "def mean_pooling(output, mask):\n",
    "    embeddings = output.last_hidden_state\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    return torch.sum(embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "sentence_emb = mean_pooling(outputs, inputs['attention_mask'])\n",
    "print(sentence_emb.shape)  # 应输出 torch.Size([1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10556/2450663772.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')  # 使用CPU避免GPU内存问题\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "检查点文件: /home/lxz/PubMedbert/finetune_bert.pth\n",
      "文件包含的键: ['bert_state', 'projection_state', 'rel_projection_state']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 加载检查点文件\n",
    "checkpoint_path = \"/home/lxz/PubMedbert/finetune_bert.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')  \n",
    "\n",
    "# 打印检查点的基本信息\n",
    "print(\"=\"*50)\n",
    "print(f\"检查点文件: {checkpoint_path}\")\n",
    "print(f\"文件包含的键: {list(checkpoint.keys())}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语义相似度: 0.9858\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 加载本地模型\n",
    "model_path = \"/home/lxz/PubMedbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "# 定义均值池化函数\n",
    "def mean_pooling(output, mask):\n",
    "    embeddings = output.last_hidden_state\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    return torch.sum(embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# 待比较的术语\n",
    "terms = [\"Primary cultured cells\", \"primary cultured cell\"]\n",
    "# terms = [\"Primary cultured cells\", \"neural crest derived fibroblast\"]\n",
    "# 编码文本\n",
    "inputs = tokenizer(terms, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取句向量\n",
    "embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
    "\n",
    "# 计算余弦相似度\n",
    "sim = F.cosine_similarity(embeddings[0], embeddings[1], dim=0).item()\n",
    "print(f\"语义相似度: {sim:.4f}\")  # 输出范围[-1,1]，越接近1越相似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取完成！共 7310 个三元组，5626 种细胞类型\n",
      "关系统计: {'is_a': 3818, 'exact_synonyms': 2270, 'related_synonyms': 527, 'broad_synonyms': 262, 'disjoint_from': 32, 'develops from': 372, 'develops into': 14, 'synapsed to': 15}\n",
      "\n",
      "前3个三元组示例:\n",
      "{'head': 'primary cultured cell', 'tail': 'cultured cell', 'relation': 0}\n",
      "{'head': 'neural crest derived fibroblast', 'tail': 'fibroblast', 'relation': 0}\n",
      "{'head': 'neuronal receptor cell', 'tail': 'sensory neuron', 'relation': 0}\n",
      "\n",
      "细胞类型示例: ['serous cell of epithelium of terminal bronchiole', 'somatocrinin secreting cell', 'non-nucleated secondary lens fibre', 'S2b fibroblast', 'cuboidal GC']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_triplets_and_cell_types(file_path):\n",
    "    \"\"\"\n",
    "    从文件中提取三元组和所有细胞类型\n",
    "    返回:\n",
    "        - triplets: [(head, relation, tail), ...]\n",
    "        - cell_types: set() 所有唯一的细胞类型\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    cell_types = set()\n",
    "    relation_counter = defaultdict(int)\n",
    "    \n",
    "    # 匹配模式：head relation tail\n",
    "    pattern = re.compile(r\"(.+?)\\s+(is_a|disjoint_from|exact_synonyms|broad_synonyms|\"\n",
    "                         r\"related_synonyms|develops from|develops into|synapsed to)\\s+(.+)\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                head, relation, tail = match.groups()\n",
    "                triplets.append((head, relation, tail))\n",
    "                cell_types.update([head, tail])\n",
    "                relation_counter[relation] += 1\n",
    "    \n",
    "    # print(f\"提取完成！共 {len(triplets)} 个三元组，{len(cell_types)} 种细胞类型\")\n",
    "    # print(\"关系统计:\", dict(relation_counter))\n",
    "    \n",
    "    return triplets, cell_types\n",
    "\n",
    "# 使用示例\n",
    "file_path = \"/mnt/c/Users/94903/Desktop/cellannotation/PubMedbert/triples.txt\"\n",
    "triplets, cell_types = extract_triplets_and_cell_types(file_path)\n",
    "\n",
    "# 转换为您需要的格式\n",
    "relation_mapping = {\n",
    "    \"is_a\": 0,\n",
    "    \"disjoint_from\": 1,\n",
    "    \"exact_synonyms\": 2,\n",
    "    \"broad_synonyms\": 3,\n",
    "    \"related_synonyms\": 4,\n",
    "    \"develops from\": 5,\n",
    "    \"develops into\": 6,\n",
    "    \"synapsed to\": 7\n",
    "}\n",
    "\n",
    "data = []\n",
    "for h, r, t in triplets:\n",
    "    data.append({\n",
    "        \"head\": h,\n",
    "        \"tail\": t,\n",
    "        \"relation\": relation_mapping[r]\n",
    "    })\n",
    "\n",
    "# # 结果验证\n",
    "# print(\"\\n前3个三元组示例:\")\n",
    "# for item in data[:3]:\n",
    "#     print(item)\n",
    "\n",
    "# print(\"\\n细胞类型示例:\", list(cell_types)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取到 7310 个三元组\n",
      "发现 5626 种唯一细胞类型\n",
      "\n",
      "前3个三元组示例:\n",
      "primary cultured cell is_a cultured cell\n",
      "neural crest derived fibroblast is_a fibroblast\n",
      "neuronal receptor cell is_a sensory neuron\n",
      "\n",
      "部分细胞类型示例: ['360 nm-cone', '5-HT neuron', '5-HT secreting cell', '5-Hydroxytryptamine secreting cell', '5-hydroxytryptamine neuron']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_triplets_and_types(file_path):\n",
    "    \"\"\"\n",
    "    提取文件中所有三元组和唯一细胞类型\n",
    "    返回:\n",
    "        triplets: [(head, relation, tail), ...]\n",
    "        cell_types: set() 所有唯一的细胞类型\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    cell_types = set()\n",
    "    \n",
    "    # 匹配所有预定义关系（使用正则表达式中的命名捕获组）\n",
    "    pattern = re.compile(\n",
    "        r\"(?P<head>.+?)\\s+\"\n",
    "        r\"(?P<relation>is_a|disjoint_from|exact_synonyms|broad_synonyms|\"\n",
    "        r\"related_synonyms|develops from|develops into|synapsed to)\\s+\"\n",
    "        r\"(?P<tail>.+)\"\n",
    "    )\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # 非空行处理\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    head = match.group('head')\n",
    "                    relation = match.group('relation')\n",
    "                    tail = match.group('tail')\n",
    "                    \n",
    "                    triplets.append((head, relation, tail))\n",
    "                    cell_types.update([head, tail])\n",
    "    \n",
    "    return triplets, cell_types\n",
    "\n",
    "# 使用示例\n",
    "file_path = \"/mnt/c/Users/94903/Desktop/cellannotation/PubMedbert/triples.txt\"\n",
    "triplets, cell_types = extract_triplets_and_types(file_path)\n",
    "\n",
    "# 转换为目标格式（如果需要）\n",
    "relation_mapping = {\n",
    "    \"is_a\": 0,\n",
    "    \"disjoint_from\": 1,\n",
    "    \"exact_synonyms\": 2,\n",
    "    \"broad_synonyms\": 3,\n",
    "    \"related_synonyms\": 4,\n",
    "    \"develops from\": 5,\n",
    "    \"develops into\": 6,\n",
    "    \"synapsed to\": 7\n",
    "}\n",
    "\n",
    "data = [{\n",
    "    \"head\": h,\n",
    "    \"tail\": t,\n",
    "    \"relation\": relation_mapping[r]\n",
    "} for h, r, t in triplets]\n",
    "\n",
    "# 查看结果样例\n",
    "print(f\"提取到 {len(triplets)} 个三元组\")\n",
    "print(f\"发现 {len(cell_types)} 种唯一细胞类型\")\n",
    "print(\"\\n前3个三元组示例:\")\n",
    "for h, r, t in triplets[:3]:\n",
    "    print(f\"{h} {r} {t}\")\n",
    "\n",
    "print(\"\\n部分细胞类型示例:\", sorted(cell_types)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2716438/2595024645.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  finetuned_model.load_state_dict(torch.load(\"/home/lxz/PubMedbert/finetune_bert.pth\", map_location=device)[\"bert_state\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding相似度: 0.9130\n",
      "是否一致: 否\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "text = \"primary cultured cell\"\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lxz/PubMedbert\")\n",
    "base_model = AutoModel.from_pretrained(\"/home/lxz/PubMedbert\").to(device)\n",
    "finetuned_model = AutoModel.from_pretrained(\"/home/lxz/PubMedbert\").to(device)\n",
    "finetuned_model.load_state_dict(torch.load(\"/home/lxz/PubMedbert/finetune_bert.pth\", map_location=device)[\"bert_state\"])\n",
    "\n",
    "# 生成embedding\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "with torch.no_grad():\n",
    "    base_embedding = base_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    finetuned_embedding = finetuned_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# 计算相似度\n",
    "similarity = cosine_similarity(base_embedding, finetuned_embedding)[0][0]\n",
    "\n",
    "print(f\"Embedding相似度: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10556/2438006018.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/home/lxz/PubMedbert/finetune_bert.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文本对: 'common myeloid progenitor' vs 'megakaryocyte-erythroid progenitor cell'\n",
      "原始BERT相似度: 0.8692\n",
      "投影后相似度: 0.9289\n",
      "差异变化: +0.0597\n",
      "\n",
      "文本对: 'common myeloid progenitor' vs 'common lymphoid progenitor'\n",
      "原始BERT相似度: 0.7871\n",
      "投影后相似度: 0.8739\n",
      "差异变化: +0.0867\n",
      "\n",
      "文本对: 'neuronal receptor cell' vs 'sensory neuron'\n",
      "原始BERT相似度: 0.8858\n",
      "投影后相似度: 0.9239\n",
      "差异变化: +0.0381\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ProjectedBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(0.1)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, **inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)  # 平均池化\n",
    "        return self.projection(pooled)  # 通过完整投影层\n",
    "\n",
    "# 初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lxz/PubMedbert\")\n",
    "\n",
    "# 1. 加载基础BERT模型（无投影层）\n",
    "base_model = AutoModel.from_pretrained(\"/home/lxz/PubMedbert\").to(device)\n",
    "\n",
    "# 2. 加载微调模型（带完整投影层）\n",
    "finetuned_model = ProjectedBERT(\n",
    "    AutoModel.from_pretrained(\"/home/lxz/PubMedbert\")\n",
    ").to(device)\n",
    "\n",
    "# 加载检查点参数\n",
    "checkpoint = torch.load(\"/home/lxz/PubMedbert/finetune_bert.pth\", map_location=device)\n",
    "\n",
    "# 精确加载参数\n",
    "finetuned_model.bert.load_state_dict(checkpoint['bert_state'])\n",
    "finetuned_model.projection.load_state_dict(checkpoint['projection_state'])\n",
    "\n",
    "# 生成embedding的函数（推理时关闭dropout）\n",
    "def get_embedding(model, text):\n",
    "    model.eval()  # 确保dropout被关闭\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        # 处理不同模型的输出差异\n",
    "        if isinstance(model, ProjectedBERT):\n",
    "            output = model(**inputs)  # 直接获取投影后的张量\n",
    "        else:\n",
    "            output = model(**inputs).last_hidden_state.mean(dim=1)  # 原始BERT需要手动池化\n",
    "        return output.cpu().numpy()  # 转换为numpy数组\n",
    "\n",
    "# 计算相似度\n",
    "def compare_similarity(model, text_a, text_b):\n",
    "    emb_a = get_embedding(model, text_a)\n",
    "    emb_b = get_embedding(model, text_b)\n",
    "    return cosine_similarity(emb_a, emb_b)[0][0]\n",
    "\n",
    "# 使用示例\n",
    "text_pairs = [\n",
    "    (\"common myeloid progenitor\", \"megakaryocyte-erythroid progenitor cell\"),\n",
    "    (\"common myeloid progenitor\", \"common lymphoid progenitor\"),\n",
    "    (\"neuronal receptor cell\", \"sensory neuron\")\n",
    "]\n",
    "\n",
    "for text1, text2 in text_pairs:\n",
    "    base_sim = compare_similarity(base_model, text1, text2)\n",
    "    tuned_sim = compare_similarity(finetuned_model, text1, text2)\n",
    "    \n",
    "    print(f\"\\n文本对: '{text1}' vs '{text2}'\")\n",
    "    print(f\"原始BERT相似度: {base_sim:.4f}\")\n",
    "    print(f\"投影后相似度: {tuned_sim:.4f}\")\n",
    "    print(f\"差异变化: {tuned_sim - base_sim:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triplets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 构建关系图\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m relation_graph \u001b[38;5;241m=\u001b[39m build_relation_graph(\u001b[43mtriplets\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_contrastive_types\u001b[39m(celltype):\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    输入一个细胞类型，返回一个需要拉近的类型和一个需要拉远的类型\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    返回: (close_type, far_type)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'triplets' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_and_organize_triplets(file_path):\n",
    "    \"\"\"\n",
    "    提取并组织三元组，按头实体分类，支持按类型和关系查询\n",
    "    \n",
    "    参数:\n",
    "        file_path: 包含三元组的文本文件路径\n",
    "        \n",
    "    返回:\n",
    "        tuple: (triplet_dict, all_cell_types)\n",
    "            - triplet_dict: 字典 {head_entity: [(relation, tail_entity), ...]}\n",
    "            - all_cell_types: 所有唯一细胞类型的集合\n",
    "    \"\"\"\n",
    "    # 正则表达式匹配三元组\n",
    "    pattern = re.compile(\n",
    "        r\"(?P<head>.+?)\\s+\"\n",
    "        r\"(?P<relation>is_a|disjoint_from|exact_synonyms|broad_synonyms|\"\n",
    "        r\"related_synonyms|develops_from|develops_into|synapsed_to)\\s+\"\n",
    "        r\"(?P<tail>.+)\"\n",
    "    )\n",
    "    \n",
    "    # 使用defaultdict自动初始化空列表\n",
    "    triplet_dict = defaultdict(list)\n",
    "    all_cell_types = set()\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    head = match.group('head')\n",
    "                    relation = match.group('relation')\n",
    "                    tail = match.group('tail')\n",
    "                    \n",
    "                    # 添加到字典和集合\n",
    "                    triplet_dict[head].append((relation, tail))\n",
    "                    all_cell_types.update([head, tail])\n",
    "    \n",
    "    return dict(triplet_dict), all_cell_types\n",
    "\n",
    "# 示例使用\n",
    "file_path = \"/home/lxz/PubMedbert/triples.txt\"\n",
    "triplet_dict, cell_types = extract_and_organize_triplets(file_path)\n",
    "\n",
    "# --- 查询功能示例 ---\n",
    "def get_relations_by_head(head_entity, relation_type=None):\n",
    "    \"\"\"\n",
    "    根据头实体和可选的关系类型查询三元组\n",
    "    \n",
    "    参数:\n",
    "        head_entity: 要查询的头实体名称\n",
    "        relation_type: 可选，指定关系类型（如\"is_a\"）\n",
    "        \n",
    "    返回:\n",
    "        list: 匹配的三元组列表 [(relation, tail), ...]\n",
    "    \"\"\"\n",
    "    if head_entity not in triplet_dict:\n",
    "        return []\n",
    "    \n",
    "    if relation_type is None:\n",
    "        return triplet_dict[head_entity]\n",
    "    else:\n",
    "        return [(rel, tail) for rel, tail in triplet_dict[head_entity] \n",
    "                if rel == relation_type]\n",
    "\n",
    "# 示例查询1：获取所有与\"T_cell\"相关的三元组\n",
    "print(\"T_cell的所有关系:\")\n",
    "for rel, tail in get_relations_by_head(\"T cell\"):\n",
    "    print(f\"  {rel} {tail}\")\n",
    "\n",
    "# 示例查询2：只获取\"T_cell\"的\"is_a\"关系\n",
    "print(\"\\nT_cell的is_a关系:\")\n",
    "for rel, tail in get_relations_by_head(\"T cell\", \"is_a\"):\n",
    "    print(f\"  {rel} {tail}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
